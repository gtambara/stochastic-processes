{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Processes - class 1/2024 - University of Brasília\n",
    "Computational work 4 - classification\n",
    "\n",
    "Gabriel Tambara Rabelo - 241106461\n",
    "\n",
    "O presente projeto visa classificar de diferentes métodos para \n",
    "\n",
    "expectancy,mode,median,variance,skewness,kurtosis,entropy,class\n",
    "53.891967984934084,16.0,16.0,2366.491050364323,97417.68593562066,12161679.665451163,1.9169438405823693,0\n",
    "80.08158192090394,16.0,48.0,5663.520651358167,289773.10140834015,61596894.36590457,2.3554137854502386,0\n",
    "80.37126177024481,16.0,48.0,6078.010526279875,372971.9596642113,79378789.52637905,2.392316701226323,0\n",
    "\n",
    "h[0],h[1],h[2],h[3],h[4],h[5],h[6],h[7],class\n",
    "58419,11150,9625,13219,13300,450,20,17,0\n",
    "52196,7232,6733,7406,9320,9624,12177,1512,0\n",
    "\n",
    "Para uma rodada dos testes do código, foram geradas as matrizes de confusão e suas análises indicam alguns pontos de interesse quanto ao método e o conjunto específico de dados utilizado. Esses resultados obtidos não necessariamente indicam que um algoritmo é melhor que o outro, contudo, para o presente conjunto de dados e amostras, podemos definir sua classificabilidade.\n",
    "\n",
    "A seguir, têm-se os resultados de um dos testes da classificação, seguido pela sua análise.\n",
    "\n",
    "--------------------CASE1--------------------\n",
    "\n",
    "Bayes Confusion Matrix:\n",
    "[[11  0  0  0  0]\n",
    "[ 0  1  0  1  0]\n",
    "[ 0  0  3  0  0]\n",
    "[ 0  0  0  6  0]\n",
    "[ 0  0  0  0  4]]\n",
    "Bayes Accuracy per Class:\n",
    "[1.  0.5 1.  1.  1. ]\n",
    "Bayes Accuracy: 0.9615384615384616\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "QDA Confusion Matrix:\n",
    "[[11  0  0  0  0]\n",
    "[ 0  2  0  0  0]\n",
    "[ 0  0  3  0  0]\n",
    "[ 0  0  0  6  0]\n",
    "[ 0  0  0  0  4]]\n",
    "QDA Accuracy per Class:\n",
    "[1. 1. 1. 1. 1.]\n",
    "QDA Accuracy: 1.0\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "LDA Confusion Matrix:\n",
    "[[11  0  0  0  0]\n",
    "[ 0  0  0  0  2]\n",
    "[ 0  0  3  0  0]\n",
    "[ 0  0  0  6  0]\n",
    "[ 0  0  2  0  2]]\n",
    "LDA Accuracy per Class:\n",
    "[1.  0.  1.  1.  0.5]\n",
    "LDA Accuracy: 0.8461538461538461\n",
    "\n",
    "--------------------CASE2--------------------\n",
    "\n",
    "Bayes Confusion Matrix:\n",
    "[[11  0  0  0  0]\n",
    "[ 0  1  0  1  0]\n",
    "[ 0  0  3  0  0]\n",
    "[ 0  1  0  4  1]\n",
    "[ 0  0  0  0  4]]\n",
    "Bayes Accuracy per Class:\n",
    "[1.         0.5        1.         0.66666667 1.        ]\n",
    "Bayes Accuracy: 0.8846153846153846\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "QDA Confusion Matrix:\n",
    "[[11  0  0  0  0]\n",
    "[ 0  1  0  1  0]\n",
    "[ 0  0  3  0  0]\n",
    "[ 0  0  0  4  2]\n",
    "[ 0  0  0  0  4]]\n",
    "QDA Accuracy per Class:\n",
    "[1.         0.5        1.         0.66666667 1.        ]\n",
    "QDA Accuracy: 0.8846153846153846\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "LDA Confusion Matrix:\n",
    "[[11  0  0  0  0]\n",
    "[ 0  0  1  1  0]\n",
    "[ 0  0  3  0  0]\n",
    "[ 0  0  0  6  0]\n",
    "[ 0  0  1  2  1]]\n",
    "LDA Accuracy per Class:\n",
    "[1.   0.   1.   1.   0.25]\n",
    "LDA Accuracy: 0.8076923076923077\n",
    "\n",
    "Para o caso 1:\n",
    "\n",
    "O classificador Naive Bayes teve um bom desempenho para a maioria das classes, com uma precisão perfeita para 4 das 5 classes.\n",
    "A classe 2 teve um desempenho menor com precisão de 50%, indicando que há espaço para melhorias na diferenciação dessa classe.\n",
    "A precisão geral é alta, mostrando que o classificador funciona bem com esses dados, mas pode ter limitações em situações onde as classes não são bem separadas. Esse problema seria resolvido com um espaço amostral maior, o que poderia gerar melhores acurácias também para a classe problemática, porém, também poderia reduzir a acurácia da classificação das outras classes.\n",
    "\n",
    "O QDA obteve uma precisão perfeita em todas as classes.\n",
    "Isso indica que o QDA foi capaz de capturar muito bem as características dos dados, provavelmente devido à sua capacidade de modelar distribuições não lineares. No entanto, precisão perfeita pode às vezes indicar overfitting, especialmente com um conjunto de dados de teste pequeno.\n",
    "\n",
    "O LDA teve uma boa performance para a maioria das classes, mas falhou completamente para a classe 2, e teve uma precisão de 50% para a classe 4.\n",
    "A precisão geral é inferior ao Naive Bayes e QDA.\n",
    "\n",
    "Para o caso 2:\n",
    "\n",
    "O Naive Bayes apresentou uma redução na precisão geral e por classe comparado ao caso 1.\n",
    "A classe 2 ainda tem uma precisão de 50%, e a classe 4 reduziu para 66.67%.\n",
    "\n",
    "O QDA também apresentou uma redução na precisão geral e por classe comparado ao caso 1.\n",
    "A precisão para a classe 4 diminuiu para 66.67%, e a classe 2 manteve a precisão de 50%..\n",
    "\n",
    "O LDA teve uma redução significativa na precisão geral e para a classe 4 comparado ao Case 1.\n",
    "A precisão para a classe 2 é nula e a precisão para a classe 4 é de apenas 25%, indicando dificuldades significativas em separar essas classes com as características usadas.\n",
    "A precisão geral é a mais baixa entre os classificadores, indicando que o LDA é o menos eficaz com as características do caso 2.\n",
    "\n",
    "\n",
    "A seguir, segue o código equivalente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "img_size = 300\n",
    "classes = ['Alzheimer', 'COVID', 'Brazilian_seeds', 'Brazilian_leaves', 'skin_cancer']\n",
    "data_dir = './image_dataset/'\n",
    "case1_csv_filename = 'case1_statistics.csv'\n",
    "case2_csv_filename = 'case2_statistics.csv'\n",
    "reg_param = 1e-5  # Adjusted regularization parameter\n",
    "\n",
    "def load_images_from_folder(folder_path):\n",
    "    image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    images = []\n",
    "\n",
    "    for image_file in image_files:\n",
    "        img = cv.imread(os.path.join(folder_path, image_file), cv.IMREAD_GRAYSCALE)\n",
    "        ratio = img_size / img.shape[1]\n",
    "        img_resized = cv.resize(img, (img_size, int(img.shape[0] * ratio)), cv.INTER_AREA)\n",
    "        images.append(img_resized)\n",
    "\n",
    "    return images\n",
    "\n",
    "def makeHist(image, bits):\n",
    "    hist, bins = np.histogram(image.flatten(), bins=np.arange(0, 257, 2 ** (8 - bits)))\n",
    "    return hist, bins\n",
    "\n",
    "def normalizeHist(hist):\n",
    "    total_pixels = np.sum(hist)\n",
    "    normalized_hist = hist / total_pixels\n",
    "    return normalized_hist\n",
    "\n",
    "def bin_centers(bin_borders):\n",
    "    bin_center_list = (bin_borders[:-1] + bin_borders[1:]) / 2\n",
    "    bin_center_list[-1] = 255\n",
    "    return bin_center_list\n",
    "\n",
    "def expectancy(hist, bin_centers):\n",
    "    return np.sum(hist * bin_centers)\n",
    "\n",
    "def median(hist, bin_centers):\n",
    "    cumulative_freq = 0\n",
    "    for i, freq in enumerate(hist):\n",
    "        cumulative_freq += freq\n",
    "        if cumulative_freq >= 0.5:\n",
    "            return bin_centers[i]\n",
    "\n",
    "def mode(hist, bin_centers):\n",
    "    return bin_centers[np.argmax(hist)]\n",
    "\n",
    "def moment(hist, bin_centers, order, expectancy):\n",
    "    return np.sum(((bin_centers - expectancy) ** order) * hist)\n",
    "\n",
    "def entropy(hist):\n",
    "    epsilon = 1e-27\n",
    "    return -np.sum(hist * np.log2(hist + epsilon))\n",
    "\n",
    "def calculate_statistics(image, bins):\n",
    "    hist, bin_edges = makeHist(image, bins)\n",
    "    normalized_hist = normalizeHist(hist)\n",
    "    centered_bins = bin_centers(bin_edges)\n",
    "\n",
    "    var_exp = expectancy(normalized_hist, centered_bins)\n",
    "    var_median = median(normalized_hist, centered_bins)\n",
    "    var_mode = mode(normalized_hist, centered_bins)\n",
    "    var_variance = moment(normalized_hist, centered_bins, 2, var_exp)\n",
    "    var_skewness = moment(normalized_hist, centered_bins, 3, var_exp)\n",
    "    var_kurtosis = moment(normalized_hist, centered_bins, 4, var_exp)\n",
    "    var_entropy = entropy(normalized_hist)\n",
    "\n",
    "    return hist, var_exp, var_mode, var_median, var_variance, var_skewness, var_kurtosis, var_entropy\n",
    "\n",
    "def save_statistics_to_csv(images, class_label, case1_csv_filename, case2_csv_filename):\n",
    "    with open(case1_csv_filename, mode='a', newline='') as case1_file, open(case2_csv_filename, mode='a', newline='') as case2_file:\n",
    "        case1_writer = csv.writer(case1_file)\n",
    "        case2_writer = csv.writer(case2_file)\n",
    "\n",
    "        for image in images:\n",
    "            hist, var_exp, var_mode, var_median, var_variance, var_skewness, var_kurtosis, var_entropy = calculate_statistics(image, bins=8)\n",
    "\n",
    "            case_1_data = list(hist) + [class_label]\n",
    "            case_2_data = [var_exp, var_mode, var_median, var_variance, var_skewness, var_kurtosis, var_entropy, class_label]\n",
    "\n",
    "            case1_writer.writerow(case_1_data)\n",
    "            case2_writer.writerow(case_2_data)\n",
    "\n",
    "def load_data_from_csv(filename):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(filename, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            data.append([float(x) for x in row[:-1]])\n",
    "            labels.append(int(row[-1]))\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def train_test_split(data, labels, test_size=0.1):\n",
    "    np.random.seed(np.random.randint(0,99))\n",
    "    indices = np.arange(len(data))\n",
    "    np.random.shuffle(indices)\n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "    train_indices = indices[:split_idx]\n",
    "    test_indices = indices[split_idx:]\n",
    "    return data[train_indices], data[test_indices], labels[train_indices], labels[test_indices]\n",
    "\n",
    "with open(case1_csv_filename, mode='w', newline='') as case1_file, open(case2_csv_filename, mode='w', newline='') as case2_file:\n",
    "    case1_writer = csv.writer(case1_file)\n",
    "    case2_writer = csv.writer(case2_file)\n",
    "\n",
    "    case1_writer.writerow([f'h[{i}]' for i in range(8)] + ['class'])\n",
    "    case2_writer.writerow(['expectancy', 'mode', 'median', 'variance', 'skewness', 'kurtosis', 'entropy', 'class'])\n",
    "\n",
    "for class_id, class_name in enumerate(classes):\n",
    "    folder_path = os.path.join(data_dir, class_name)\n",
    "    images = load_images_from_folder(folder_path)\n",
    "    save_statistics_to_csv(images, class_id, case1_csv_filename, case2_csv_filename)\n",
    "\n",
    "data_case1, labels_case1 = load_data_from_csv(case1_csv_filename)\n",
    "data_case2, labels_case2 = load_data_from_csv(case2_csv_filename)\n",
    "\n",
    "X_train_case1, X_test_case1, y_train_case1, y_test_case1 = train_test_split(data_case1, labels_case1, test_size=0.1)\n",
    "X_train_case2, X_test_case2, y_train_case2, y_test_case2 = train_test_split(data_case2, labels_case2, test_size=0.1)\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.mean = {}\n",
    "        self.var = {}\n",
    "        self.priors = {}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            X_c = X[y == cls]\n",
    "            self.mean[cls] = np.mean(X_c, axis=0)\n",
    "            self.var[cls] = np.var(X_c, axis=0) + reg_param\n",
    "            self.priors[cls] = X_c.shape[0] / X.shape[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        for cls in self.classes:\n",
    "            prior = np.log(self.priors[cls])\n",
    "            posterior = np.sum(np.log(self._pdf(cls, x)))\n",
    "            posterior = prior + posterior\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _pdf(self, cls, x):\n",
    "        mean = self.mean[cls]\n",
    "        var = self.var[cls]\n",
    "        numerator = np.exp(- (x - mean) ** 2 / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        results = numerator / denominator\n",
    "\n",
    "        for i in range(len(results)):\n",
    "            if results[i] == 0:\n",
    "                results[i] = reg_param\n",
    "\n",
    "        return results\n",
    "\n",
    "class QuadraticDiscriminantAnalysis:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.mean = {}\n",
    "        self.covariance = {}\n",
    "        self.priors = {}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            X_c = X[y == cls]\n",
    "            self.mean[cls] = np.mean(X_c, axis=0)\n",
    "            self.covariance[cls] = np.cov(X_c, rowvar=False) + np.eye(X.shape[1]) * reg_param\n",
    "            self.priors[cls] = X_c.shape[0] / X.shape[0]\n",
    "\n",
    "        #print(\"Means:\", self.mean)\n",
    "        #print(\"Covariances:\", self.covariance)\n",
    "        #print(\"Priors:\", self.priors)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        discriminants = []\n",
    "\n",
    "        for cls in self.classes:\n",
    "            G = self._quadratic_discriminant(cls, x)\n",
    "            discriminants.append(G)\n",
    "\n",
    "        return self.classes[np.argmax(discriminants)]\n",
    "\n",
    "    def _quadratic_discriminant(self, cls, x):\n",
    "        mean = self.mean[cls]\n",
    "        covariance = self.covariance[cls]\n",
    "        inv_covariance = np.linalg.inv(covariance)\n",
    "        det_cov = np.linalg.det(covariance)\n",
    "\n",
    "        if det_cov == 0:\n",
    "            det_cov = reg_param\n",
    "\n",
    "        W_k = -0.5 * inv_covariance\n",
    "        w_k = np.dot(inv_covariance, mean)\n",
    "        w_k0 = -0.5 * np.dot(mean.T, np.dot(inv_covariance, mean)) - 0.5 * np.log(det_cov) + np.log(self.priors[cls])\n",
    "\n",
    "        discriminant = np.dot(x.T, np.dot(W_k, x)) + np.dot(w_k.T, x) + w_k0\n",
    "\n",
    "        return discriminant\n",
    "\n",
    "class LinearDiscriminantAnalysis:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.mean = {}\n",
    "        self.covariance = {}\n",
    "        self.priors = {}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            X_c = X[y == cls]\n",
    "            self.mean[cls] = np.mean(X_c, axis=0)\n",
    "            self.covariance[cls] = np.cov(X_c, rowvar=False) + np.eye(X.shape[1]) * reg_param\n",
    "            self.priors[cls] = X_c.shape[0] / X.shape[0]\n",
    "\n",
    "        #print(\"Means:\", self.mean)\n",
    "        #print(\"Covariances:\", self.covariance)\n",
    "        #print(\"Priors:\", self.priors)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        discriminants = []\n",
    "\n",
    "        for cls in self.classes:\n",
    "            G = self._linear_discriminant(cls, x)\n",
    "            discriminants.append(G)\n",
    "\n",
    "        return self.classes[np.argmax(discriminants)]\n",
    "\n",
    "    def _linear_discriminant(self, cls, x):\n",
    "        mean = self.mean[cls]\n",
    "        covariance = self.covariance[cls]\n",
    "        inv_covariance = np.linalg.inv(covariance)\n",
    "\n",
    "        w_k = np.dot(inv_covariance, mean)\n",
    "        w_k0 = -0.5 * np.dot(mean.T, np.dot(inv_covariance, mean)) + np.log(self.priors[cls])\n",
    "\n",
    "        discriminant = np.dot(w_k.T, x) + w_k0\n",
    "\n",
    "        return discriminant\n",
    "\n",
    "def confusion_matrix(y_true, y_pred, num_classes):\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for i in range(len(y_true)):\n",
    "        cm[y_true[i]][y_pred[i]] += 1\n",
    "    return cm\n",
    "\n",
    "def accuracy_per_class(cm):\n",
    "    return np.diag(cm) / np.sum(cm, axis=1)\n",
    "\n",
    "def overall_accuracy(cm):\n",
    "    return np.sum(np.diag(cm)) / np.sum(cm)\n",
    "\n",
    "data, labels = load_data_from_csv(case1_csv_filename)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "nb = NaiveBayesClassifier()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb, len(classes))\n",
    "acc_nb = accuracy_per_class(cm_nb)\n",
    "overall_acc_nb = overall_accuracy(cm_nb)\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, y_train)\n",
    "y_pred_qda = qda.predict(X_test)\n",
    "cm_qda = confusion_matrix(y_test, y_pred_qda, len(classes))\n",
    "acc_qda = accuracy_per_class(cm_qda)\n",
    "overall_acc_qda = overall_accuracy(cm_qda)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "y_pred_lda = lda.predict(X_test)\n",
    "cm_lda = confusion_matrix(y_test, y_pred_lda, len(classes))\n",
    "acc_lda = accuracy_per_class(cm_lda)\n",
    "overall_acc_lda = overall_accuracy(cm_lda)\n",
    "\n",
    "print(\"\\n--------------------CASE1--------------------\\n\")\n",
    "\n",
    "print(\"Bayes Confusion Matrix:\\n\", cm_nb)\n",
    "print(\"Bayes Accuracy per Class:\\n\", acc_nb)\n",
    "print(\"Bayes Accuracy:\", overall_acc_nb)\n",
    "\n",
    "print(\"\\n----------------------------------------\\n\")\n",
    "\n",
    "print(\"QDA Confusion Matrix:\\n\", cm_qda)\n",
    "print(\"QDA Accuracy per Class:\\n\", acc_qda)\n",
    "print(\"QDA Accuracy:\", overall_acc_qda)\n",
    "\n",
    "print(\"\\n----------------------------------------\\n\")\n",
    "\n",
    "print(\"LDA Confusion Matrix:\\n\", cm_lda)\n",
    "print(\"LDA Accuracy per Class:\\n\", acc_lda)\n",
    "print(\"LDA Accuracy:\", overall_acc_lda)\n",
    "\n",
    "data, labels = load_data_from_csv(case2_csv_filename)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "nb = NaiveBayesClassifier()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb, len(classes))\n",
    "acc_nb = accuracy_per_class(cm_nb)\n",
    "overall_acc_nb = overall_accuracy(cm_nb)\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, y_train)\n",
    "y_pred_qda = qda.predict(X_test)\n",
    "cm_qda = confusion_matrix(y_test, y_pred_qda, len(classes))\n",
    "acc_qda = accuracy_per_class(cm_qda)\n",
    "overall_acc_qda = overall_accuracy(cm_qda)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "y_pred_lda = lda.predict(X_test)\n",
    "cm_lda = confusion_matrix(y_test, y_pred_lda, len(classes))\n",
    "acc_lda = accuracy_per_class(cm_lda)\n",
    "overall_acc_lda = overall_accuracy(cm_lda)\n",
    "\n",
    "print(\"\\n--------------------CASE2--------------------\\n\")\n",
    "\n",
    "print(\"Bayes Confusion Matrix:\\n\", cm_nb)\n",
    "print(\"Bayes Accuracy per Class:\\n\", acc_nb)\n",
    "print(\"Bayes Accuracy:\", overall_acc_nb)\n",
    "\n",
    "print(\"\\n----------------------------------------\\n\")\n",
    "\n",
    "print(\"QDA Confusion Matrix:\\n\", cm_qda)\n",
    "print(\"QDA Accuracy per Class:\\n\", acc_qda)\n",
    "print(\"QDA Accuracy:\", overall_acc_qda)\n",
    "\n",
    "print(\"\\n----------------------------------------\\n\")\n",
    "\n",
    "print(\"LDA Confusion Matrix:\\n\", cm_lda)\n",
    "print(\"LDA Accuracy per Class:\\n\", acc_lda)\n",
    "print(\"LDA Accuracy:\", overall_acc_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
